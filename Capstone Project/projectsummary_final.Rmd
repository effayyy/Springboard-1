---
title: "Capstone Project Summary"
output: html_document
---

*Springboard Foundations of Data Science*  
*By Jessie Huang*  
*Mentor: Shmuel Naaman*  
*October, 18, 2016*

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background
***
Airbnb is a popular bed & breakfast option where homeowners (Hosts) can rent out their entire property or certain rooms (Listing) for travelers. With the increasing popularity of Airbnb coupled with dramatically increased rent prices in San Francisco, Iâ€™d like to understand the most important features that influence the listing price for San Francisco Airbnb listings and what a host can to do maximize their listing price.  


## Obtaining and Cleaning the Data
***
The dataset is provided by [Inside Airbnb](http://insideairbnb.com/), an independent and non-commercial website. Inside Airbnb scrapes listing data from specific cities on Airbnb's site to analyze and explore how Airbnb is being used around the world.  

I began with a dataset that contains detailed information about each listing in San Francisco that was active at the time Inside Airbnb scraped Airbnb's site for San Francisco data on July 2, 2016. The raw dataset is 8,619 rows long and 96 columns wide.  

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Load libraries
library(caret)
library(lattice)
library(randomForest)
library(ggplot2)
library(gridExtra)
library(gbm)
library(plyr)
library(dplyr)
library(knitr)

# Load csv
listings <- read.csv("~/Dropbox (Personal)/Springboard/Capstone Project/listings_final.csv")

# Remove features not used in analysis
# Check for NA values
#listings[!complete.cases(listings),]
# Observation 4496 has NA in neighborhood_overview and notes. Remove these features.
listings$neighborhood_overview <- NULL
listings$notes <- NULL

# Remove unused features
listings$X <- NULL
listings$X.1 <- NULL
listings$X.2 <- NULL
listings$name <- NULL
listings$summary <- NULL
listings$space <- NULL
listings$description <- NULL
listings$transit <- NULL 
listings$access <- NULL
listings$interaction <- NULL
listings$house_rules <- NULL
listings$host_since <- NULL
listings$host_about <- NULL
listings$street <- NULL
listings$amenities <- NULL
listings$first_review <- NULL
listings$last_review <- NULL
```

After cleaning the data and removing irrelevant features from the dataset, there are 40 predictor variables to work with.  

## Data Analysis and Feature Selection
***

I used the One-Way ANOVA and correlation coefficient statistical tests to determine whether a feature is statistically significant to the price. The One-Way ANOVA is used when comparing two or more group means (categorical features) on a continuous dependent variable (log_price). For numerical features, I used the correlation coefficient to test for statistical significance. The correlation test is a normalized measurement of how two numerical features are linearly related.  

However, a problem I ran into was the skewed distribution of the price feature. The One-Way ANOVA test is designed for features that have a normal distribution. After creating a new feature in the dataset that takes the price feature on a log scale, the distribution appears normal.  

```{r echo=FALSE, message=FALSE, warning=FALSE}
grid.arrange(
ggplot(data=listings, aes(x = price)) + 
  geom_histogram(bins = 30) +
  theme(panel.background=element_blank(),
        panel.grid.minor=element_blank(),
        axis.line=element_line(colour=NA),
        axis.line.x=element_line(colour="grey80")) +
  labs(title = "Positively Skewed Distribution", x="Price (Normal Scale)", y="# of Listings"),
ggplot(data=listings, aes(x = price)) + 
  geom_histogram(bins = 30) +
  theme(panel.background=element_blank(),
        panel.grid.minor=element_blank(),
        axis.line=element_line(colour=NA),
        axis.text.x = element_text(hjust = 1),
        axis.line.x=element_line(colour="grey80")) +
  scale_x_log10() +
  labs(title = "Normal Distribution", x="Price (Log Scale)", y="# of Listings"), 
ncol=2)
```

After the exploratory and statistical analysis stage, I ended up with the below list of features that were statistically significant and interesting to explore for the predictive model. The next step is to remove features that are closely correlated with each other and may cause overfitting or create bias in the results.  

1. neighbourhood_cleansed
2. room_type
3. accommodates
4. host_listings_count
5. minimum_nights
6. is_dorm
7. bathrooms
8. beds
9. bedrooms
10. bed_type
11. number_of_reviews
12. reviews_per_month
13. review_scores_rating
14. cancellation_policy  

### Feature Engineering

Some features were removed because they are too similar to each other. While they may improve the r-squared value, but is actually causing overfitting. Some examples are number_of_reviews, reviews_per_month, and review_scores_rating are most likely very closely related to each other. While including these do help with boosting the r-ssquared value, there is clearly some bias. This is confirmed because removing reviews_per_month only decreases the r-squared value by less than 0.01. I decided to  remove reviews_per_month as it is too closely correlated to number_of_reviews where cor = 0.5671.  

Similarly, the features: bathrooms, beds, bedrooms, and bed_type are likely very closely correlated to accommodates. After performing the correlation test on each of these features to *accommodates*, all of the features have a strong linear relationship where cor > 0.7, with the exception of *bathrooms* where cor = 0.4398.  

The below scatterplot helps to confirm that we are not overfitting the model and there is little correlation between review_scores_rating/number_of_reviews and bathrooms/accommodates:    

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Correlation tests of  similar features
cor.test(listings$review_scores_rating, listings$number_of_reviews)
cor.test(listings$accommodates, listings$bathrooms)
#summary(lm(accommodates ~ bed_type, data = listings))

grid.arrange(
ggplot(data = listings, aes(x=review_scores_rating, y=number_of_reviews)) +
  geom_point(alpha = 0.2) +
  geom_smooth(method = "lm", se = FALSE, size=1) +
  theme(panel.background = element_blank(),
        axis.line.x = element_line(colour="grey80"),
        axis.line.y = element_line(colour="grey80")), 
ggplot(data = listings, aes(x=accommodates, y=bathrooms)) +
  geom_point(alpha = 0.2) +
  geom_smooth(method = "lm", se = FALSE, size=1) +
  theme(panel.background = element_blank(),
        axis.line.x = element_line(colour="grey80"),
        axis.line.y = element_line(colour="grey80")),
ncol=2)
```

Now that we have the final list of features to work with for predictive modeling, the next step is to understand which features actually have a high probability of influencing the listing price.  

## Predictive Modeling
***

In this seciton, we will fit and test various models to determine which model to use for future data on San Francisco Airbnb listings. To do so, the dataset will be cut in to a training set (80%) and testing set (20%). I will use the random forest and generalized boosted regression (GBM) models for predictive modeling on the training set. Based on the results of the models, I will take the highest performing model and verify its accuracy on unseen data with the testing set.  


### Random Forest Modeling 
***

The random forest model is averaging multiple decision trees that are trained on different parts of the same training set. The goal is to overcome the overfitting problem of individual decision trees. An overfitted model fits the noise in the data rather than the actual underlying relationships among the variables. Overfitting usually occurs when a model is unnecessarily complex.  

After feature engineering, the final iteration generates an r-squared value of 0.6330219 with a training size of 6,800 rows with the following features:  

```{r echo=FALSE, message=FALSE, warning=FALSE}
### Random Forest Model
# set the seed to make your partition reproducible
set.seed(123)

# features for model
splitlistings <- listings[c("log_price", 
                            "neighbourhood_cleansed", 
                            "room_type", 
                            "accommodates", 
                            "host_listings_count", 
                            "minimum_nights", 
                            "bathrooms", 
                            "number_of_reviews", 
                            "review_scores_rating", 
                            "cancellation_policy")]
inTraining <- createDataPartition(splitlistings$log_price, p = .8, list = FALSE)

# save the training and testing sets as data frames
train_1 <- listings[ inTraining,]
test_1  <- listings[-inTraining,]

# fit the randomforest model
model <- train(log_price ~ neighbourhood_cleansed + room_type + accommodates +
                 host_listings_count + minimum_nights + bathrooms + 
                 number_of_reviews + review_scores_rating + cancellation_policy, 
               data=train_1, 
               method="rf", 
               metric="RMSE", 
               tuneGrid=expand.grid(.mtry=3), 
               ntree=250,
               importance=TRUE)

# what are the important variables (via permutation)
vi <- varImp(model, type=1)
plot(vi, top=10)

# predict the outcome of the training data
predicted_tr <- predict(model, newdata=train_1, select = -c(log_price))
actual_tr <- train_1$log_price
rsq_tr <- 1-sum((actual_tr-predicted_tr)^2)/sum((actual_tr-mean(actual_tr))^2)

# predict the outcome of the testing data
predicted <- predict(model, newdata=test_1, select = -c(log_price))
actual <- test_1$log_price
rsq <- 1-sum((actual-predicted)^2)/sum((actual-mean(actual))^2)
```

Based on the results of the random forest model, the top ten important variables in the model is given regarding each class of the feature. We can see that the listing price is influenced by the room type and features related to the listing accommodation size, which includes the number of people the listing can accommodate and the number of bathrooms available. The total number of reviews the listing has received also influences the listing price. However, we can see that the different San Francisco neighborhood breakdowns are also significant in influencing the price.  

Additionally, the model returns an r-squared value of 0.6726716 for the training set and the r-squared value for the testing set is 0.6275013.   

Because the dataset I am working with is quite small, the next step is to determine the validity of the random forest model to ensure that there was no bias in the way the training and testing data were split and to protect against overfitting. I will use the k-fold and grid search cross-validation techniques to evaluate the predictive models and also to tune the model parameters.  

#### Random Forest Model Boosting: K-Fold and Grid Search Cross Validation  

```{r echo=FALSE, message=FALSE, warning=FALSE}
# ensure results are repeatable
set.seed(123)
inTraining <- createDataPartition(listings$log_price, p = .8, list = FALSE)
training <- listings[ inTraining,]
testing  <- listings[-inTraining,]

# Manual Grid Search
control <- trainControl(method="repeatedcv", 
                        number=10, 
                        repeats=1, 
                        search="grid")
tunegrid <- expand.grid(.mtry=c(2,3))
modellist <- list()
metric = "RMSE"

#For loop of gridsearch
for (ntree in c(50, 100, 150, 200, 250)) {
	set.seed(123)
	fit <- train(log_price ~ neighbourhood_cleansed + room_type + accommodates + host_listings_count + minimum_nights + bathrooms + number_of_reviews + review_scores_rating + cancellation_policy, 
	             data=training, 
	             method="rf", 
	             metric=metric, 
	             tuneGrid=tunegrid, 
	             trControl=control, 
	             ntree=ntree)
	key <- toString(ntree)
	modellist[[key]] <- fit
}

#### compare results
results <- resamples(modellist)
summary(results)

# predict the outcome of the training data
predicted_tr <- predict(modellist[['250']], newdata=training, select = -c(log_price))
actual_tr <- training$log_price
rsq_tr <- 1-sum((actual_tr-predicted_tr)^2)/sum((actual_tr-mean(actual_tr))^2)

# predict the outcome of the testing data
predicted <- predict(modellist[['250']], newdata=testing, select = -c(log_price))
actual <- testing$log_price
rsq <- 1-sum((actual-predicted)^2)/sum((actual-mean(actual))^2)
```

After boosting the random forest model with k-fold and grid search cross validation, the r-squared values only improve slightly for both the training and testing sets. With the base model, the r-squared value for the testing set is 0.6275013. After tuning the model, the testing set r-squared value only improves to 0.6333874. We can also see from the scatterplot that there isn't much difference.  

```{r echo=FALSE, message=FALSE, warning=FALSE}
## Scatterplot of model accuracy
# Assign actual and predicted to testing set
df <- data.frame(predicted)
df$actual <- actual

# Scatterplot of test set of actual and predicted
ggplot(df, aes(x=predicted, y=actual)) + 
  geom_point(alpha=0.2) + 
  geom_smooth(method = "lm", se = FALSE, size=1) +
  theme(panel.background = element_blank(),
          axis.line.x = element_line(colour="grey80"),
          axis.line.y = element_line(colour="grey80")) +
  labs(title="Accuracy of Boosted Random Forest Model", x="Prediction Set", y="Test Set")
```


### Generalized Boosted Modeling (GBM)  
***

The next machine learning method that we try is the GBM model. This model fits to the data via boosted decision trees. GBM will train many decision trees sequentially, each time increasing the weight of data points predicted incorrectly the previous time and decreasing those that were predicted correctly. By combining the many trees, this produces a stronger predictor.  

#### GBM: K-Fold and Grid Search Cross Validation  

```{r echo=FALSE, message=FALSE, warning=FALSE}
# ensure results are repeatable
set.seed(123)
inTraining <- createDataPartition(listings$log_price, p = .8, list = FALSE)
training <- listings[ inTraining,]
testing  <- listings[-inTraining,]

# manual grid search - define control
control <- trainControl(method="repeatedcv", number=10, repeats=1, search="grid")

# define grid
gbmGrid <- expand.grid(n.trees = c(50,100,150,200,250), 
                       interaction.depth = c(2,5,7,10),
                       shrinkage = 0.05,
                       n.minobsinnode = 10)

# GBM model
fit.gbm <- train(log_price ~ neighbourhood_cleansed + room_type + accommodates + host_listings_count + minimum_nights + bathrooms + number_of_reviews + review_scores_rating + cancellation_policy, 
  data = training, 
  method="gbm",
  tuneGrid = gbmGrid,
  trControl=control, 
  verbose=FALSE)

print(fit.gbm)
plot(fit.gbm)

# predict the outcome of the training data
predicted_tr <- predict(fit.gbm, newdata=training, select = -c(log_price))
actual_tr <- training$log_price
rsq_tr <- 1-sum((actual_tr-predicted_tr)^2)/sum((actual_tr-mean(actual_tr))^2)

# predict the outcome of the testing data
predicted <- predict(fit.gbm, newdata=testing, select = -c(log_price))
actual <- testing$log_price
rsq <- 1-sum((actual-predicted)^2)/sum((actual-mean(actual))^2)

## Scatterplot of model accuracy
# Assign actual and predicted to testing set
df <- data.frame(predicted)
df$actual <- actual

# Scatterplot of test set of actual and predicted
ggplot(df, aes(x=predicted, y=actual)) + 
  geom_point(alpha=0.2) + 
  geom_smooth(method = "lm", se = FALSE, size=1) +
  theme(panel.background = element_blank(),
          axis.line.x = element_line(colour="grey80"),
          axis.line.y = element_line(colour="grey80")) +
  labs(title="Accuracy of GBM Model", x="Prediction Set", y="Test Set")
```

The GBM model performs much better than the random forest model, with an r-squared value of 0.685973. The final model plot tells that the overall error converge at around 250 trees. However, at even at 200 trees, tree depths of 5, 7, and 10 converge to approximately the same root-mean-square error (RMSE) so it's possible to speed up the algorithm by tuning the number of trees down to 200 for similar results.  


## Conclusion & Next Steps    
***

The aim of this project is to build an accurate prediction model for San Francisco Airbnb listing prices based on what the host can offer in terms of their home and the host's history and reputation on the site. To achieve this, I applied statistical analysis to create a list of features to explore during the modeling phase and with feature engineering I narrowed the list down by eliminating the features that are too closely correlated and may cause overfitting. I then used the random forest and GBM models to train the dataset and selected GBM as the prediction model due to its relative higher accuracy in the cross validation.  

During the predictive modeling stage, I found that:  

1. The most important feature that influences the price is the number of people the listing can offer. Other features related to the accommodation size are just as significance in influencing the price. location of the listing in San Francisco.  
2. The host's history on Airbnb is also important, as the number of reviews the listing has received also influences the listing price.
3. The neighborhood is surprising not as important as you would expect - it is likely because there are many neighborhoods and some neighborhoods do not have enough data.  

However, we must remember some limitations:  
1. The dataset is small, so there are probably some inaccuracies with the model.  
2. We cannot conclude whether the statistically significant features help to increase the likelihood of a listing to being booked.  
3. No booking data included in the dataset, so we cannot understand trends throughout the calendar year or most frequently booked listings.  

For future extension of this project, it would be interesting to explore what helps a listing to be ranked higher based on its offerings and key words. Additionally, how does a host earn the badges that would encourage a potential guest to book the listing? Finding whether there's a pttern between review scores, key words, photographs would be interesting to understand what potential items helps a listing actually get booked after a host has priced their listing competitively.