---
title: "Mini Project: Linear Regression"
output: html_document
---

#  Introduction
## Learning objectives:
     • Learn the R formula interface
     • Specify factor contrasts to test specific hypotheses
     • Perform model comparisons
     • Run and interpret variety of regression models in R

## Load the states data

```{r}
# load the states data
states.data <- readRDS("~/Documents/Springboard/Curriculum Projects/linear_regression/dataSets/states.rds") 

#get labels
states.info <- data.frame(attributes(states.data)[c("names", "var.labels")])

#look at last few labels
tail(states.info, 8)
```

## Linear regression
Examine the data before fitting models. Start by examining the data to check for problems.

```{r }
# summary of expense and csat columns, all rows
sts.ex.sat <- subset(states.data, select = c("expense", "csat"))
summary(sts.ex.sat)

# correlation between expense and csat
cor(sts.ex.sat)
```

## Plot the data before fitting models
Plot the data to look for multivariate outliers, non-linear relationships etc.

```{r}
# scatter plot of expense vs csat
plot(sts.ex.sat)
```

## Linear regression example

• Linear regression models can be fit with the `lm()' function
• For example, we can use `lm' to predict SAT scores based on per-pupal expenditures:

```{r}
# Fit our regression model
sat.mod <- lm(csat ~ expense, data=states.data)

# Summarize and print the results
summary(sat.mod) # show regression coefficients table

```

Many people find it surprising that the per-capita expenditure on students is negatively related to SAT scores. The beauty of multiple regression is that we can try to pull these apart. What would the association between expense and SAT scores be if there were no difference among the states in the percentage of students taking the SAT?

```{r}
summary(lm(csat ~ expense + percent, data = states.data))
```

## The lm class and methods
OK, we fit our model. Now what?
• Examine the model object:
```{r}
class(sat.mod)
names(sat.mod)
methods(class = class(sat.mod))[1:9]
```

• Use function methods to get more information about the fit

```{r}
confint(sat.mod)
hist(residuals(sat.mod))
```

## Linear Regression Assumptions

• Ordinary least squares regression relies on several assumptions, including that the residuals are normally distributed and homoscedastic, the errors are independent and the relationships are linear.

• Investigate these assumptions visually by plotting your model:

```{r}
par(mar = c(4, 4, 2, 2), mfrow = c(1, 2)) #optional
plot(sat.mod, which = c(1, 2)) # "which" argument optional
```

## Comparing models
Do congressional voting patterns predict SAT scores over and above expense? Fit two models and compare them:

```{r}
# fit another model, adding house and senate as predictors
sat.voting.mod <-  lm(csat ~ expense + house + senate,
                      data = na.omit(states.data))
sat.mod <- update(sat.mod, data=na.omit(states.data))

# compare using the anova() function
anova(sat.mod, sat.voting.mod)
coef(summary(sat.voting.mod))

```

## Exercise: least squares regression

Use the /states.rds/ data set. Fit a model predicting energy consumed per capita (energy) from the percentage of residents living in metropolitan areas (metro). Be sure to
1. Examine/plot the data before fitting the model
2. Print and interpret the model `summary'
3. `plot' the model to look for deviations from modeling assumptions

Select one or more additional predictors to add to your model and repeat steps 1-3. Is this model significantly better than the model with /metro/ as the only predictor?

```{r}
# summary of energy and metro columns, all rows
sts.en.met <- subset(states.data, select = c("energy", "metro"))
summary(sts.en.met)
# There is an NA. Replace with median.
sts.en.met$energy[is.na(sts.en.met$energy)] <- median(sts.en.met$energy, na.rm=TRUE)
sts.en.met$metro[is.na(sts.en.met$metro)] <- median(sts.en.met$metro, na.rm=TRUE)

# correlation between energy and metro
cor(sts.en.met)

# Plot the data before fitting the model
plot(sts.en.met)

# Fit regression model
model <- lm(metro ~ energy, 
            data=na.omit(states.data))

# Summarize and print the results
summary(model) # show regression coefficients table

# `plot' the model to look for deviations from modeling assumptions
plot(model)

#get labels
states.info

# fit another model, adding waste and green as predictors
model2 <- lm(metro ~ energy + house + senate,
                      data = na.omit(states.data))
model2 <- update(model2, data=na.omit(states.data))

# compare using the anova() function
anova(model, model2)
coef(summary(model2))

```


## Interactions and factors: Modeling interactions

Interactions allow us assess the extent to which the association between one predictor and the outcome depends on a second predictor. For example: Does the association between expense and SAT scores depend on the median income in the state?

```{r}
# Add the interaction to the model
sat.expense.by.percent <- lm(csat ~ expense*income,
                             data=states.data) 
# Show the results
  coef(summary(sat.expense.by.percent)) # show regression coefficients table

```

## Regression with categorical predictors

Let's try to predict SAT scores from region, a categorical variable. Note that you must make sure R does not think your categorical variable is numeric.

```{r}
# make sure R knows region is categorical
str(states.data$region)
states.data$region <- factor(states.data$region)
# Add region to the model
sat.region <- lm(csat ~ region,
                 data=states.data) 
# Show the results
coef(summary(sat.region)) # show regression coefficients table
anova(sat.region) # show ANOVA table

##   Again, *make sure to tell R which variables are categorical by converting them to factors!*

```

## Setting factor reference groups and contrasts

In the previous example we use the default contrasts for region. The default in R is treatment contrasts, with the first level as the reference. We can change the reference group or use another coding scheme using the `C' function.

```{r}
# print default contrasts
contrasts(states.data$region)
# change the reference group
coef(summary(lm(csat ~ C(region, base=4),
                data=states.data)))
# change the coding scheme
coef(summary(lm(csat ~ C(region, contr.helmert),
                data=states.data)))

##   See also `?contrasts', `?contr.treatment', and `?relevel'.

```

## Exercise: interactions and factors

Use the states data set.

1. Add on to the regression equation that you created in exercise 1 by generating an interaction term and testing the interaction.

2. Try adding region to the model. Are there significant differences across the four regions?

```{r}
# Generate an interaction term and test the interaction
mod.en.metro.by.waste <- lm(energy ~ metro * waste, data = states.data)

# Add region to the model
mod.en.region <- lm(energy ~ metro * waste + region, data = states.data)
anova(mod.en.region)
```

